{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/twitter_credentials.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through to gather around 2k tweets and filter out retweets and replies \n",
    "num_needed = 2000\n",
    "tweet_list = []\n",
    "last_id = -1 # id of last tweet seen\n",
    "while len(tweet_list) < num_needed:\n",
    "    try:\n",
    "        new_tweets = api.search(q = 'climate change AND -filter:retweets AND -filter:replies', count = 100, max_id = str(last_id - 1), tweet_mode ='extended', lang ='en')\n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"Error\", e)\n",
    "        break\n",
    "    else:\n",
    "        if not new_tweets:\n",
    "            print(\"Could not find any more tweets!\")\n",
    "            break\n",
    "        tweet_list.extend(new_tweets)\n",
    "        last_id = new_tweets[-1].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pandas dataframe using data points screen_name and full_text from the tweet\n",
    "t = [[tweet.user.screen_name,tweet.full_text] for tweet in tweet_list]\n",
    "tweet_text = pd.DataFrame(data=t, columns=['user', \"text\"])\n",
    "tweet_text.to_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to remove https links, special characters, punctuations, convert the text to lowercase\n",
    "# I used regular expressions here\n",
    "def fnc_remove_spec_characters(df,col):\n",
    "    listtext =[]\n",
    "    for i in range(len(df)):\n",
    "        clean_text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", df[col][i])\n",
    "        listtext.append(clean_text.lower())\n",
    "    return(listtext)\n",
    "# After cleaning up , a new column is added to the dataframe named as cleaned_text by calling the function\n",
    "tweet_text['cleaned_text'] = fnc_remove_spec_characters(tweet_text,\"text\")\n",
    "tweet_text[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize keywords for people who believe in climate change vs people who deny climate change\n",
    "\n",
    "search_words_believers =['is real','action','urgent','actonclimate','climateemergency','climateaction','urges','urge','greennewdeal','stop','policy','threat','terrifying', 'impacts', 'climateactionnow','earthday','earthdayeveryday','chaotic','extinctionrebellion','climatecatastrophe','climatechangethefacts','suffering']\n",
    "search_words_nonbelievers =['is not real','hoax','misinformation','lies','fake','fakenews','refuse','suppressing','misleading','false','climatehoax', 'skeptic','denied', 'deny']\n",
    "\n",
    "#Define a function to categorize tweets using the keywords above and create a new column named \"sentiment\" to be added to the dataframe\n",
    "def fnc_identify_sentiment(df,col):\n",
    "    sentiment_list =[]\n",
    "    for i in range(len(df)):\n",
    "        text = df[col][i]\n",
    "        if any(word in text for word in search_words_believers):\n",
    "            sentiment_list.append('Believer')\n",
    "        elif any(word in text for word in search_words_nonbelievers):\n",
    "            sentiment_list.append('Denier')\n",
    "        else:\n",
    "            sentiment_list.append('Not determinable')\n",
    "    return(sentiment_list)  \n",
    "tweet_text['sentiment'] = fnc_identify_sentiment(tweet_text,'cleaned_text') \n",
    "\n",
    "#Create 100 tweets file as a subset of the cleaned file to be submitted for the assignment - this 100 tweets file has username, cleaned_text and the sentiment column\n",
    "final_tweet_df = tweet_text[['user', 'cleaned_text', 'sentiment']].copy()\n",
    "#Create cleaned file with 2096 tweets to be imported into R for further analysis\n",
    "final_tweet_df.to_csv(\"finalcleaned.csv\")\n",
    "\n",
    "#create file using to_csv\n",
    "final_tweet_df[0:100].to_csv(\"100tweetsfile.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
